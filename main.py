from datetime import datetime, timedelta
import time
import os
import pandas as pd
from dotenv import load_dotenv
from get import get_campaign_data
from csv_handler import process_campaigns_data, save_to_csv
from drive_handler import (
    get_google_drive_service,
    create_folder_if_not_exists,
    upload_csv_to_drive,
)
from twilio_utils import send_notification_with_fallback

# Load environment variables
load_dotenv()


def main():
    print("\nüîÑ Starting PubPlus campaign data collection...")

    # Initialize Google Drive and Sheets services
    try:
        drive_service, sheets_service = get_google_drive_service()
        print("‚úÖ Successfully connected to Google services")
    except Exception as e:
        error_message = f"‚ùå Error connecting to Google services: {e}"
        print(error_message)
        print("Please check your Google API credentials and internet connection.")
        send_notification_with_fallback(f"ALERT: {error_message}")
        return

    # Create or get the main campaign_data folder in Google Drive
    try:
        drive_folder_id = create_folder_if_not_exists(drive_service, "campaign_data")
        print(f"‚ÑπÔ∏è Using Google Drive folder: campaign_data (ID: {drive_folder_id})")
    except Exception as e:
        error_message = f"‚ùå Error accessing Google Drive folder: {e}"
        print(error_message)
        send_notification_with_fallback(f"ALERT: {error_message}")
        return

    # Create directory for data if it doesn't exist
    output_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "campaign_data")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"‚ÑπÔ∏è Created local directory: {output_dir}")

    # Define the single CSV file path using os.path.join
    filename = os.path.join(output_dir, "pubplus_campaign_data.csv")

    # Get current date and calculate start date (30 days ago)
    today = datetime.now()
    start_date = today - timedelta(days=7)
    print(f"‚ÑπÔ∏è Fetching data from {start_date.strftime('%Y-%m-%d')} to {today.strftime('%Y-%m-%d')}")

    all_campaigns = []
    successful_days = 0
    failed_days = 0
    empty_days = 0

    # Fetch data for each day in the last 30 days
    current_date = start_date
    while current_date <= today:
        date_str = current_date.strftime("%Y-%m-%d")
        # Use 5-hour intervals to match the working API pattern
        for hour in range(0, 24, 5):
            start_hour = hour
            end_hour = min(hour + 5, 24)
            
            start_datetime = f"{date_str} {start_hour:02d}:00:00"
            end_datetime = f"{date_str} {end_hour:02d}:00:00"

            print(f"\n‚ÑπÔ∏è Fetching data for {date_str} ({start_hour:02d}:00 - {end_hour:02d}:00)...")

            response_data = get_campaign_data(start_datetime, end_datetime)

            if response_data:
                campaigns_list = process_campaigns_data(response_data)
                if campaigns_list and len(campaigns_list) > 0:
                    for campaign in campaigns_list:
                        campaign["date"] = date_str
                    all_campaigns.extend(campaigns_list)
                    print(f"‚úÖ Successfully processed {len(campaigns_list)} campaigns")
                else:
                    print(f"‚ö†Ô∏è No campaign data found for this time period")

            # Add delay between requests to avoid rate limiting
            if hour < 20:  # Don't sleep after the last request of the day
                print("‚ÑπÔ∏è Waiting before next request...")
                time.sleep(2)  # Add 2 second delay between requests

        if any(campaign["date"] == date_str for campaign in all_campaigns):
            successful_days += 1
        elif response_data is None:
            failed_days += 1
        else:
            empty_days += 1

        current_date += timedelta(days=1)

        # Add delay between days to avoid rate limiting
        if current_date <= today:
            print("‚ÑπÔ∏è Waiting before next day...")
            time.sleep(5)  # Add 5 second delay between days

    # Summary of data collection
    print(f"\nüìä Data collection summary:")
    print(f"  ‚úÖ Successful days: {successful_days}")
    print(f"  ‚ö†Ô∏è Empty days: {empty_days}")
    print(f"  ‚ùå Failed days: {failed_days}")
    print(f"  üìã Total campaigns collected: {len(all_campaigns)}")

    if all_campaigns:
        # Save/update local CSV
        save_to_csv(all_campaigns, filename)
        print(f"‚úÖ Saved data to local CSV: {filename}")

        # Upload to Google Drive
        try:
            print(f"‚ÑπÔ∏è Uploading data to Google Drive...")
            file_id = upload_csv_to_drive(drive_service, sheets_service, filename, drive_folder_id)

            if file_id:
                success_message = f"‚úÖ Data successfully updated in Google Drive spreadsheet"
                print(success_message)
                # Send success notification
                send_notification_with_fallback(
                    f"SUCCESS: PubPlus data collection complete. Updated {successful_days} days of data. ({empty_days} empty, {failed_days} failed)"
                )
            else:
                error_message = "‚ùå Failed to update Google Drive spreadsheet - file not found"
                print(error_message)
                send_notification_with_fallback(f"ALERT: {error_message}")
        except Exception as e:
            error_message = f"‚ùå Error uploading to Google Drive: {e}"
            print(error_message)
            send_notification_with_fallback(f"ALERT: {error_message}")
    else:
        error_message = "‚ö†Ô∏è No data to upload to Google Drive"
        print(error_message)
        send_notification_with_fallback(f"WARNING: {error_message}")

    print("\n‚úÖ Data collection process complete!")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        error_message = f"‚ùå CRITICAL ERROR: {e}"
        print(error_message)
        send_notification_with_fallback(f"CRITICAL ERROR: {error_message}")
